{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the model was trained on GoogleColab, this is the code for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c_kp22sfdpnz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scrapping training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0uN0zeGgggAn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "names = []\n",
        "\n",
        "for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "            'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n",
        "    url = f'https://vardai.vlkk.lt/sarasas/{key}/'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    male_links = soup.find_all('a', class_='names_list__links names_list__links--man')\n",
        "    for link in male_links:\n",
        "        names.append({'name': link.text, 'gender': 'male'})\n",
        "\n",
        "    female_links = soup.find_all('a', class_='names_list__links names_list__links--woman')\n",
        "    for link in female_links:\n",
        "        names.append({'name': link.text, 'gender': 'female'})\n",
        "\n",
        "df = pd.DataFrame(names)\n",
        "df.to_csv('names_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XJtuy69sigIc"
      },
      "outputs": [],
      "source": [
        "class NameDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        # Load CSV file \n",
        "        data = pd.read_csv(csv_file)\n",
        "        self.names = data['name'].values\n",
        "        self.genders = data['gender'].values\n",
        "\n",
        "        # Create character set and mappings\n",
        "        self.chars = sorted(list(set(''.join(self.names) + ' ')))\n",
        "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "        # Map genders to integers (0 for male, 1 for female)\n",
        "        self.gender_to_int = {'male': 0, 'female': 1}\n",
        "        self.int_to_gender = {0: 'male', 1: 'female'}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        name = self.names[idx] + ' '  # Add padding character (space) at the end\n",
        "        gender = self.genders[idx]\n",
        "\n",
        "        encoded_name = [self.char_to_int[char] for char in name]\n",
        "        encoded_gender = self.gender_to_int[gender]\n",
        "\n",
        "        return torch.tensor(encoded_name), torch.tensor(encoded_gender)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = NameDataset('/content/names_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gxyNJR-wjI-3"
      },
      "outputs": [],
      "source": [
        "def pad_collate(batch):\n",
        "    names, genders = zip(*batch)\n",
        "\n",
        "    padded_seqs = pad_sequence(names, batch_first=True, padding_value=0)\n",
        "\n",
        "    input_seq = padded_seqs[:, :-1]\n",
        "    target_seq = padded_seqs[:, 1:]\n",
        "\n",
        "    genders = torch.stack(genders)\n",
        "\n",
        "    return input_seq, target_seq, genders\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TQtRSjGajQJ-"
      },
      "outputs": [],
      "source": [
        "class MinimalTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion, gender_size):\n",
        "        super(MinimalTransformer, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gender_embed = nn.Embedding(gender_size, embed_size)  # Embedding for gender\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, gender):\n",
        "        # Embed gender and add it to the input embedding\n",
        "        gender_emb = self.gender_embed(gender).unsqueeze(1).expand(-1, x.size(1), -1)  # Repeat gender embedding for each timestep\n",
        "        positions = torch.arange(0, x.size(1)).unsqueeze(0)\n",
        "        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :] + gender_emb\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "byNG1DXkr631"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train() \n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for _, (input_seq, target_seq, gender) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq, gender) \n",
        "            loss = criterion(output.transpose(1, 2), target_seq)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        average_loss = total_loss / batch_count\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bINXL-L_sLJP",
        "outputId": "f9eb2dcc-cf4a-4da5-92d5-824d923b9e4a"
      },
      "outputs": [],
      "source": [
        "model = MinimalTransformer(\n",
        "    vocab_size=dataset.vocab_size,\n",
        "    embed_size=128,\n",
        "    num_heads=8,\n",
        "    forward_expansion=4,\n",
        "    gender_size=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O7Y4SdEjYC9",
        "outputId": "bbcf5a30-4801-48b8-ce7b-ddfb0722a747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 1.259397326486384\n",
            "Epoch 2, Average Loss: 1.2493830662471033\n",
            "Epoch 3, Average Loss: 1.2500016917824275\n",
            "Epoch 4, Average Loss: 1.2487285853374617\n",
            "Epoch 5, Average Loss: 1.2438808934019487\n",
            "Epoch 6, Average Loss: 1.2470497547873396\n",
            "Epoch 7, Average Loss: 1.2499032272651733\n",
            "Epoch 8, Average Loss: 1.240061286409853\n",
            "Epoch 9, Average Loss: 1.235540660473669\n",
            "Epoch 10, Average Loss: 1.2398626076374129\n",
            "Epoch 11, Average Loss: 1.2360095355821692\n",
            "Epoch 12, Average Loss: 1.234760382194293\n",
            "Epoch 13, Average Loss: 1.2351019568594077\n",
            "Epoch 14, Average Loss: 1.2338980902796206\n",
            "Epoch 15, Average Loss: 1.231826863741215\n",
            "Epoch 16, Average Loss: 1.2326393221677998\n",
            "Epoch 17, Average Loss: 1.2316390134600312\n",
            "Epoch 18, Average Loss: 1.2252049957339473\n",
            "Epoch 19, Average Loss: 1.2304602632880681\n",
            "Epoch 20, Average Loss: 1.229437719456292\n",
            "Epoch 21, Average Loss: 1.2323143359700681\n",
            "Epoch 22, Average Loss: 1.2272004732501365\n",
            "Epoch 23, Average Loss: 1.2292890247148958\n",
            "Epoch 24, Average Loss: 1.2219053770713655\n",
            "Epoch 25, Average Loss: 1.2246245902046384\n",
            "Epoch 26, Average Loss: 1.2254279333612192\n",
            "Epoch 27, Average Loss: 1.2260221417242358\n",
            "Epoch 28, Average Loss: 1.2291162805594946\n",
            "Epoch 29, Average Loss: 1.223446906790903\n",
            "Epoch 30, Average Loss: 1.2299902493303472\n",
            "Epoch 31, Average Loss: 1.2275862300348848\n",
            "Epoch 32, Average Loss: 1.2229526417528687\n",
            "Epoch 33, Average Loss: 1.232634899880104\n",
            "Epoch 34, Average Loss: 1.226565484237294\n",
            "Epoch 35, Average Loss: 1.2247167583039626\n",
            "Epoch 36, Average Loss: 1.2230224901508437\n",
            "Epoch 37, Average Loss: 1.2256653603357759\n",
            "Epoch 38, Average Loss: 1.224627880239675\n",
            "Epoch 39, Average Loss: 1.217917502868788\n",
            "Epoch 40, Average Loss: 1.2251905271187131\n",
            "Epoch 41, Average Loss: 1.223424089520345\n",
            "Epoch 42, Average Loss: 1.2232990938684214\n",
            "Epoch 43, Average Loss: 1.2220645033323718\n",
            "Epoch 44, Average Loss: 1.2179638357030544\n",
            "Epoch 45, Average Loss: 1.2248413489741299\n",
            "Epoch 46, Average Loss: 1.2203173347612615\n",
            "Epoch 47, Average Loss: 1.2208882605605447\n",
            "Epoch 48, Average Loss: 1.2179627588143933\n",
            "Epoch 49, Average Loss: 1.220783526247198\n",
            "Epoch 50, Average Loss: 1.2204177608131892\n",
            "Epoch 51, Average Loss: 1.2214948539677344\n",
            "Epoch 52, Average Loss: 1.2234405382819797\n",
            "Epoch 53, Average Loss: 1.2199444926303367\n",
            "Epoch 54, Average Loss: 1.2146797319174756\n",
            "Epoch 55, Average Loss: 1.2189310544093135\n",
            "Epoch 56, Average Loss: 1.2154028373273464\n",
            "Epoch 57, Average Loss: 1.2123943343464094\n",
            "Epoch 58, Average Loss: 1.2275480406557617\n",
            "Epoch 59, Average Loss: 1.220982192297698\n",
            "Epoch 60, Average Loss: 1.218245972286571\n",
            "Epoch 61, Average Loss: 1.2121460444842402\n",
            "Epoch 62, Average Loss: 1.22284430006276\n",
            "Epoch 63, Average Loss: 1.2194796009026025\n",
            "Epoch 64, Average Loss: 1.216953077570723\n",
            "Epoch 65, Average Loss: 1.2160214974946184\n",
            "Epoch 66, Average Loss: 1.218717199540421\n",
            "Epoch 67, Average Loss: 1.214713895744957\n",
            "Epoch 68, Average Loss: 1.2170273242260627\n",
            "Epoch 69, Average Loss: 1.2152828668417195\n",
            "Epoch 70, Average Loss: 1.2136384854203628\n",
            "Epoch 71, Average Loss: 1.214711304474254\n",
            "Epoch 72, Average Loss: 1.2143248640030269\n",
            "Epoch 73, Average Loss: 1.2133119881388699\n",
            "Epoch 74, Average Loss: 1.2087697518672869\n",
            "Epoch 75, Average Loss: 1.2137535348711277\n",
            "Epoch 76, Average Loss: 1.2132930015857983\n",
            "Epoch 77, Average Loss: 1.214397883697932\n",
            "Epoch 78, Average Loss: 1.2103326516189123\n",
            "Epoch 79, Average Loss: 1.2098851253392668\n",
            "Epoch 80, Average Loss: 1.2171218107811548\n",
            "Epoch 81, Average Loss: 1.219974254666581\n",
            "Epoch 82, Average Loss: 1.208559184677516\n",
            "Epoch 83, Average Loss: 1.2109956812010452\n",
            "Epoch 84, Average Loss: 1.2118696982681516\n",
            "Epoch 85, Average Loss: 1.2137913164413965\n",
            "Epoch 86, Average Loss: 1.2197325321525454\n",
            "Epoch 87, Average Loss: 1.2179873470261162\n",
            "Epoch 88, Average Loss: 1.212226492614143\n",
            "Epoch 89, Average Loss: 1.2073963991266936\n",
            "Epoch 90, Average Loss: 1.2173625685480745\n",
            "Epoch 91, Average Loss: 1.210223250945095\n",
            "Epoch 92, Average Loss: 1.2126416198820935\n",
            "Epoch 93, Average Loss: 1.218477245847227\n",
            "Epoch 94, Average Loss: 1.2078334323973523\n",
            "Epoch 95, Average Loss: 1.2135663944270771\n",
            "Epoch 96, Average Loss: 1.214747502869768\n",
            "Epoch 97, Average Loss: 1.2040067958266367\n",
            "Epoch 98, Average Loss: 1.209120907095581\n",
            "Epoch 99, Average Loss: 1.2129502150381035\n",
            "Epoch 100, Average Loss: 1.2131140234442097\n",
            "Epoch 101, Average Loss: 1.2156963777165168\n",
            "Epoch 102, Average Loss: 1.2080881176258735\n",
            "Epoch 103, Average Loss: 1.2093097082239836\n",
            "Epoch 104, Average Loss: 1.2107483949585867\n",
            "Epoch 105, Average Loss: 1.215983761157914\n",
            "Epoch 106, Average Loss: 1.2148598043814949\n",
            "Epoch 107, Average Loss: 1.2149081746108918\n",
            "Epoch 108, Average Loss: 1.215978394148378\n",
            "Epoch 109, Average Loss: 1.2128480642209412\n",
            "Epoch 110, Average Loss: 1.2137107512225276\n",
            "Epoch 111, Average Loss: 1.2099896247679065\n",
            "Epoch 112, Average Loss: 1.2132280494855798\n",
            "Epoch 113, Average Loss: 1.209768944342617\n",
            "Epoch 114, Average Loss: 1.2103154739372344\n",
            "Epoch 115, Average Loss: 1.2083221640982647\n",
            "Epoch 116, Average Loss: 1.2078849630864712\n",
            "Epoch 117, Average Loss: 1.2065766055593377\n",
            "Epoch 118, Average Loss: 1.210528445338072\n",
            "Epoch 119, Average Loss: 1.2144997423816575\n",
            "Epoch 120, Average Loss: 1.2076972558093164\n",
            "Epoch 121, Average Loss: 1.2142838296211755\n",
            "Epoch 122, Average Loss: 1.2151536917969172\n",
            "Epoch 123, Average Loss: 1.2117571639920413\n",
            "Epoch 124, Average Loss: 1.2105478140205264\n",
            "Epoch 125, Average Loss: 1.213515112758154\n",
            "Epoch 126, Average Loss: 1.2078217816918264\n",
            "Epoch 127, Average Loss: 1.210145985656105\n",
            "Epoch 128, Average Loss: 1.2127534254737522\n",
            "Epoch 129, Average Loss: 1.2107203065642256\n",
            "Epoch 130, Average Loss: 1.2042427114818408\n",
            "Epoch 131, Average Loss: 1.212049796647234\n",
            "Epoch 132, Average Loss: 1.2143554451908518\n",
            "Epoch 133, Average Loss: 1.2102947621477451\n",
            "Epoch 134, Average Loss: 1.208783582265198\n",
            "Epoch 135, Average Loss: 1.2108623625732693\n",
            "Epoch 136, Average Loss: 1.2115700709018782\n",
            "Epoch 137, Average Loss: 1.2110205280922146\n",
            "Epoch 138, Average Loss: 1.2071427896088762\n",
            "Epoch 139, Average Loss: 1.2092027386186623\n",
            "Epoch 140, Average Loss: 1.2119684445528174\n",
            "Epoch 141, Average Loss: 1.2050400409302693\n",
            "Epoch 142, Average Loss: 1.2037426699762759\n",
            "Epoch 143, Average Loss: 1.2072513030451748\n",
            "Epoch 144, Average Loss: 1.2045493844469546\n",
            "Epoch 145, Average Loss: 1.2088688965371475\n",
            "Epoch 146, Average Loss: 1.2030264447800256\n",
            "Epoch 147, Average Loss: 1.210471206031769\n",
            "Epoch 148, Average Loss: 1.2095380689315645\n",
            "Epoch 149, Average Loss: 1.2055772045855466\n",
            "Epoch 150, Average Loss: 1.2124171947302083\n",
            "Epoch 151, Average Loss: 1.2096644614053809\n",
            "Epoch 152, Average Loss: 1.209489694461521\n",
            "Epoch 153, Average Loss: 1.2097593623658884\n",
            "Epoch 154, Average Loss: 1.2090731399803765\n",
            "Epoch 155, Average Loss: 1.205221654869351\n",
            "Epoch 156, Average Loss: 1.2102658223257705\n",
            "Epoch 157, Average Loss: 1.2034980930358525\n",
            "Epoch 158, Average Loss: 1.2057779075599941\n",
            "Epoch 159, Average Loss: 1.2043239994953743\n",
            "Epoch 160, Average Loss: 1.2070885675226746\n",
            "Epoch 161, Average Loss: 1.210482428903165\n",
            "Epoch 162, Average Loss: 1.207692333596497\n",
            "Epoch 163, Average Loss: 1.2030683223437886\n",
            "Epoch 164, Average Loss: 1.203988050048059\n",
            "Epoch 165, Average Loss: 1.2063610822315745\n",
            "Epoch 166, Average Loss: 1.2070997322029746\n",
            "Epoch 167, Average Loss: 1.2081383360704414\n",
            "Epoch 168, Average Loss: 1.2093019973147998\n",
            "Epoch 169, Average Loss: 1.209617952113095\n",
            "Epoch 170, Average Loss: 1.2019034532219053\n",
            "Epoch 171, Average Loss: 1.210013564631873\n",
            "Epoch 172, Average Loss: 1.200793397049659\n",
            "Epoch 173, Average Loss: 1.2037503097368323\n",
            "Epoch 174, Average Loss: 1.2049621242308335\n",
            "Epoch 175, Average Loss: 1.2067891286295864\n",
            "Epoch 176, Average Loss: 1.2083478456900525\n",
            "Epoch 177, Average Loss: 1.2123444624569104\n",
            "Epoch 178, Average Loss: 1.208478727123954\n",
            "Epoch 179, Average Loss: 1.2049662268208892\n",
            "Epoch 180, Average Loss: 1.208180155207517\n",
            "Epoch 181, Average Loss: 1.2081272246338162\n",
            "Epoch 182, Average Loss: 1.209994770792633\n",
            "Epoch 183, Average Loss: 1.2072734832763672\n",
            "Epoch 184, Average Loss: 1.209835038119154\n",
            "Epoch 185, Average Loss: 1.2045138374618862\n",
            "Epoch 186, Average Loss: 1.2049364280323738\n",
            "Epoch 187, Average Loss: 1.2080838272694072\n",
            "Epoch 188, Average Loss: 1.2099693975900945\n",
            "Epoch 189, Average Loss: 1.2067697125932444\n",
            "Epoch 190, Average Loss: 1.210059618290234\n",
            "Epoch 191, Average Loss: 1.2066116721733757\n",
            "Epoch 192, Average Loss: 1.2018346555619372\n",
            "Epoch 193, Average Loss: 1.203458346397038\n",
            "Epoch 194, Average Loss: 1.2094862727308462\n",
            "Epoch 195, Average Loss: 1.2052759433923503\n",
            "Epoch 196, Average Loss: 1.2099477069180002\n",
            "Epoch 197, Average Loss: 1.2084602715940815\n",
            "Epoch 198, Average Loss: 1.2008782262387483\n",
            "Epoch 199, Average Loss: 1.200890907185822\n",
            "Epoch 200, Average Loss: 1.203669901893073\n"
          ]
        }
      ],
      "source": [
        "train_model(model, dataloader, epochs=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "26h3HJLSjdGG"
      },
      "outputs": [],
      "source": [
        "def sample(model, dataset, start_str='a', max_length=20, temperature=1.0, gender='male'):\n",
        "    assert temperature > 0, \"Temperature must be greater than 0\"\n",
        "    model.eval()  \n",
        "    with torch.no_grad():\n",
        "\n",
        "        chars = [dataset.char_to_int[c] for c in start_str]\n",
        "        input_seq = torch.tensor(chars).unsqueeze(0)  # Add batch dimension\n",
        "        gender_tensor = torch.tensor([dataset.gender_to_int[gender]])  # Gender encoding\n",
        "\n",
        "        output_name = start_str\n",
        "        for _ in range(max_length - len(start_str)):\n",
        "            output = model(input_seq, gender_tensor)\n",
        "\n",
        "            logits = output[0, -1] / temperature\n",
        "            probabilities = torch.softmax(logits, dim=0)\n",
        "\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.int_to_char[next_char_idx]\n",
        "\n",
        "            if next_char == ' ': \n",
        "                break\n",
        "\n",
        "            output_name += next_char\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n",
        "\n",
        "        return output_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDxtXseJ-HwA",
        "outputId": "caba76aa-89e6-42ab-aafe-c00ebd707537"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Conservative male names:\")\n",
        "for _ in range(5):\n",
        "    print(sample(model, dataset, start_str='R', temperature=0.5, gender='male')) \n",
        "\n",
        "print(\"\\nCreative female names:\")\n",
        "for _ in range(5):\n",
        "    print(sample(model, dataset, start_str='S', temperature=1.5, gender='female'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OrbvoIh2jdn-"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'namesformer_model.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
